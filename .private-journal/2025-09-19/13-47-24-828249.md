---
title: "1:47:24 PM - September 19, 2025"
date: 2025-09-19T11:47:24.828Z
timestamp: 1758282444828
---

## Project Notes

Successfully implemented Concept Constraints game to replace the "too easy" Odd One Out game. This provides much better discernment training.

Key improvements in the Concept Constraints game:
- Tests AI boundary understanding rather than just multiple choice scenarios
- Interactive challenge-response mechanism where users can question Claude's examples
- Professional scenarios that directly apply to work contexts
- Three-tier evaluation system: Valid/Edge Case/Invalid
- Forces deeper thinking about whether AI truly understands constraints

Technical implementation highlights:
- Complex state management for examples, challenges, and responses
- Multi-step API interaction: initial examples → user challenges → Claude defenses
- Dynamic UI that shows challenge history and Claude's responses
- Evaluation system that tracks user assessments of each example
- 5 pre-built professional scenarios plus custom scenario creation

The challenge-response mechanism is key - it's not enough for Claude to provide examples, users must actively test the boundaries. For example:
- Claude: "Online course X meets 'under 2 hours' constraint"  
- User challenge: "That course is actually 3 hours long"
- Claude response: Defense or acknowledgment

This mirrors real professional AI use where you need to verify AI claims and test understanding of your requirements. Much more valuable than just picking odd items from a list.

Architecture decision: Used nested challenge state to track the conversation flow while maintaining clean separation between examples and their associated challenges/responses.
